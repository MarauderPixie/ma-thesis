  @Manual{R4,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2022},
    url = {https://www.R-project.org/},
  }

@article{Schad2021,
   abstract = {Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.},
   author = {Daniel J. Schad and Bruno Nicenboim and Paul-Christian Bürkner and Michael Betancourt and Shravan Vasishth},
   month = {3},
   title = {Workflow Techniques for the Robust Use of Bayes Factors},
   url = {http://arxiv.org/abs/2103.08744},
   year = {2021},
}
@article{Chen2017,
   abstract = {Objective: In intervention research, the decision to continue developing a new program or treatment is dependent on both the change-inducing potential of a new strategy (i.e., its effect size) and the methods used to measure change, including the size of samples. This article describes a Bayesian approach to determining sample sizes in the sequential development of interventions. Description: Because sample sizes are related to the likelihood of detecting program effects, large samples are preferred. But in the design and development process that characterizes intervention research, smaller scale studies are usually required to justify more costly, larger scale studies. We present 4 scenarios designed to address common but complex questions regarding sample-size determination and the risk of observing misleading (e.g., false-positive) findings. From a Bayesian perspective, this article describes the use of decision rules composed of different target probabilities and prespecified effect sizes. Monte-Carlo simulations are used to demonstrate a Bayesian approach—which tends to require smaller samples than the classical frequentist approach—in the development of interventions from one study to the next.},
   author = {Ding Geng Chen and Mark W. Fraser},
   doi = {10.1086/693433},
   issn = {1948822X},
   issue = {3},
   journal = {Journal of the Society for Social Work and Research},
   keywords = {Bayesian,Intervention research,Monte-Carlo simulation,Research design,Sample size},
   month = {9},
   pages = {457-470},
   publisher = {University of Chicago Press},
   title = {A bayesian approach to sample size estimation and the decision to continue program development in intervention research},
   volume = {8},
   year = {2017},
}
@article{CK17,
   abstract = {Reference point approaches have dominated the study of categorization for decades by explaining classification learning in terms of similarity to stored exemplars or averages of exemplars. The most successful reference point models are firmly grounded in the associative learning tradition—treating categorization as a stimulus generalization process based on inverse exponential distance in psychological space augmented by a dimensional selective attention mechanism. We present experiments that pose a significant challenge to popular reference point accounts which explain categorization in terms of stimulus generalization from exemplars, prototypes, or adaptive clusters. DIVA, a similarity-based alternative to the reference point framework, provides a successful account of the human data. These findings suggest that a successful psychology of categorization may need to look beyond stimulus generalization and toward a view of category learning as the induction of a richer model of the data.},
   author = {Nolan Conaway and Kenneth J. Kurtz},
   doi = {10.3758/s13423-016-1208-1},
   issn = {15315320},
   issue = {4},
   journal = {Psychonomic Bulletin and Review},
   keywords = {Categorization,Classification learning,Concepts,Formal models,Generalization,Neural network models,Stimulus generalization theory},
   month = {12},
   pages = {1312-1323},
   pmid = {27981437},
   publisher = {Springer New York LLC},
   title = {Similar to the category, but not the exemplars: A study of generalization},
   volume = {24},
   year = {2017},
}
@report{Shepard1961,
   author = {Roger N Shepard and Carl I Hovland and Herbert M Jenkins},
   issue = {13},
   title = {Psychological Monographs: General and Applied LEARNING AND MEMORIZATION OF CLASSIFICATIONS1},
   volume = {75},
   year = {1961},
}
@article{Kurtz2013,
   abstract = {The findings of Shepard, Hovland, and Jenkins (1961) on the relative ease of learning 6 elemental types of 2-way classifications have been deeply influential 2 times over: 1st, as a rebuke to pure stimulus generalization accounts, and again as the leading benchmark for evaluating formal models of human category learning. The litmus test for models is the ability to simulate an observed advantage in learning a category structure based on an exclusive-or (XOR) rule over 2 relevant dimensions (Type 11) relative to category structures that have no perfectly predictive cue or cue combination (including the linearly-separable Type IV). However, a review of the literature reveals that a Type II advantage over Type IV is found only under highly specific experimental conditions. We investigate when and why a Type II advantage exists to determine the appropriate benchmark for models and the psychological theories they represent. A series of 8 experiments link particular conditions of learning to outcomes ranging from a traditional Type 11 advantage to compelling non-differences and reversals (i.e.. Type IV advantage). Common interpretations of the Type II advantage as either a broad-based phenomenon of human learning or as strong evidence for an attention-mediated similarity-based account are called into question by our findings. Finally, a role for verbalization in the category learning process is supported. The seminal research of Shepard, Hovland, and Jenkins (1961) shifted the direction of scientific thinking on concept learning when it first appeared. The work has more than stood the test of time-50 years later the most important benchmark for evaluating formal accounts of classification learning is the observed ordering of the ease of learning of the SHJ (for short) elemental category},
   author = {Kenneth J Kurtz and Kimery R Levering and Roger D Stanton and Joshua Romero and Steven N Morris and J Kurtz},
   doi = {10.1037/a0029I78},
   issue = {2},
   journal = {Mathy \& Brad-metz},
   keywords = {category learning,formal models of category learning,selective attention,task ^instructions,verbalization},
   pages = {552-572},
   publisher = {Pitt},
   title = {Human Learning of Elemental Category Structures: Revisiting the Classic Result of Shepard, Hovland, and Jenkins (1961)},
   volume = {39},
   year = {2013},
}
@article{Barr2013,
   abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
   author = {Dale J Barr and Roger Levy and Christoph Scheepers and Harry J Tily},
   doi = {https://doi.org/10.1016/j.jml.2012.11.001},
   issn = {0749-596X},
   issue = {3},
   journal = {Journal of Memory and Language},
   keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
   pages = {255-278},
   title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
   volume = {68},
   url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
   year = {2013},
}
@article{Andras2014,
   abstract = {In management research, empirical data are often analyzed using p-value null hypothesis significance testing (pNHST). Here we outline the conceptual and practical advantages of an alternative analysis method: Bayesian hypothesis testing and model selection using the Bayes factor. In contrast to pNHST, Bayes factors allow researchers to quantify evidence in favor of the null hypothesis. Also, Bayes factors do not require adjustment for the intention with which the data were collected. The use of Bayes factors is demonstrated through an extended example for hierarchical regression based on the design of an experiment recently published in the Journal of Management. This example also highlights the fact that p values overestimate the evidence against the null hypothesis, misleading researchers into believing that their findings are more reliable than is warranted by the data.},
   author = {Sandra Andraszewicz and Benjamin Scheibehenne and Jörg Rieskamp and Raoul Grasman and Josine Verhagen and Eric-Jan Wagenmakers},
   doi = {10.1177/0149206314560412},
   issn = {0149-2063},
   issue = {2},
   journal = {Journal of Management},
   month = {12},
   note = {doi: 10.1177/0149206314560412},
   pages = {521-543},
   publisher = {SAGE Publications Inc},
   title = {An Introduction to Bayesian Hypothesis Testing for Management Research},
   volume = {41},
   url = {https://doi.org/10.1177/0149206314560412},
   year = {2014},
}
@article{Gronau2020,
   abstract = {Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve highdimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.},
   author = {Quentin F Gronau and Henrik Singmann and Eric-Jan Wagenmakers},
   doi = {10.18637/jss.v092.i10},
   issue = {10},
   journal = {Journal of Statistical Software},
   month = {2},
   pages = {1 - 29},
   title = {bridgesampling: An R Package for Estimating Normalizing Constants},
   volume = {92},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v092i10},
   year = {2020},
}
@article{Schlegelmilch2021,
   author = {René Schlegelmilch and Andy J. Wills and Bettina von Helversen},
   doi = {10.1037/rev0000321},
   issn = {1939-1471},
   journal = {Psychological Review},
   month = {9},
   title = {A cognitive category-learning model of rule abstraction, attention learning, and contextual modulation.},
   year = {2021},
}
@article{Schlegelmilch2020,
   author = {René Schlegelmilch and Bettina von Helversen},
   doi = {10.1037/xge0000747},
   issn = {1939-2222},
   issue = {10},
   journal = {Journal of Experimental Psychology: General},
   month = {10},
   pages = {1823-1854},
   title = {The influence of reward magnitude on stimulus memory and stimulus generalization in categorization decisions.},
   volume = {149},
   year = {2020},
}

@article{Chen2010,
   author = {Henian Chen and Patricia Cohen and Sophie Chen},
   doi = {10.1080/03610911003650383},
   issn = {0361-0918},
   issue = {4},
   journal = {Communications in Statistics - Simulation and Computation},
   month = {3},
   pages = {860-864},
   title = {How Big is a Big Odds Ratio? Interpreting the Magnitudes of Odds Ratios in Epidemiological Studies},
   volume = {39},
   year = {2010},
}

@article{Brysbaert2018,
   author = {Marc Brysbaert and Michaël Stevens},
   doi = {10.5334/joc.10},
   issn = {2514-4820},
   issue = {1},
   journal = {Journal of Cognition},
   month = {1},
   title = {Power Analysis and Effect Size in Mixed Effects Models: A Tutorial},
   volume = {1},
   year = {2018},
}

@manual{Gelman2020,
   author = {Andrew Gelman},
   title = {Prior Choice Recommendations},
   organization = {Stan Project},
   year = {2020},
   url = {https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations}
}

@article{Vehtari2021,
	doi = {10.1214/20-ba1221},  
	url = {https://doi.org/10.1214%2F20-ba1221},  
	year = 2021,
	month = {jun},  
	publisher = {Institute of Mathematical Statistics},  
	volume = {16},  
	number = {2},  
	author = {Aki Vehtari and Andrew Gelman and Daniel Simpson and Bob Carpenter and Paul-Christian Bürkner},  
	title = {Rank-Normalization, Folding, and Localization: An Improved $\hat{R}$ for Assessing Convergence of {MCMC} (with Discussion)},  
	journal = {Bayesian Analysis}
}

@article{Buerkner2017,
 title={brms: An R Package for Bayesian Multilevel Models Using Stan},
 volume={80},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
 doi={10.18637/jss.v080.i01},
 abstract={The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
 number={1},
 journal={Journal of Statistical Software},
 author={Bürkner, Paul-Christian},
 year={2017},
 pages={1–28}
}

@misc{Bates2015,
  doi = {10.48550/ARXIV.1506.04967},  
  url = {https://arxiv.org/abs/1506.04967},  
  author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},  
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Parsimonious Mixed Models},  
  publisher = {arXiv},  
  year = {2015},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Hull1920,
  title={Quantitative aspects of evolution of concepts: An experimental study.},
  author={Clark L. Hull},
  journal={The Psychological Monographs},
  year={1920},
  volume={28}
}

@article{Kruschke1992,
  title     = "{ALCOVE}: An exemplar-based connectionist model of category learning",
  author    = "Kruschke, John K.",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  99,
  number    =  1,
  pages     = "22--44",
  year      =  1992
}

@article{Love2004,
  title     = "{SUSTAIN}: a network model of category learning",
  author    = "Love, Bradley C. and Medin, Douglas L. and Gureckis, Todd M.",
  abstract  = "SUSTAIN (Supervised and Unsupervised STratified Adaptive
               Incremental Network) is a model of how humans learn categories from examples. SUSTAIN initially assumes a simple category structure. If simple solutions prove inadequate and SUSTAIN is confronted with a surprising event (e.g., it is told that a bat is a mammal instead of a bird), SUSTAIN recruits an additional cluster to represent the surprising event. Newly recruited clusters are available to explain future events and can  themselves evolve into prototypes-attractors-rules. SUSTAIN's discovery of category substructure is affected not only by the structure of the world but by the nature of the learning task and the learner's goals. SUSTAIN successfully extends category learning models to studies of inference learning, unsupervised learning, category construction, and contexts in which identification learning is faster than classification learning.",
  journal   = "Psychol. Rev.",
  publisher = "American Psychological Association (APA)",
  volume    =  111,
  number    =  2,
  pages     = "309--332",
  month     =  apr,
  year      =  2004,
  language  = "en"
}

@article{Kurtz2007,
  title     = "The divergent autoencoder ({DIVA}) model of category learning",
  author    = "Kutrz, Kenneth J",
  abstract  = "A novel theoretical approach to human category learning is proposed in which categories are represented as coordinated statistical models of the properties of the members. Key elements of the account are learning to recode inputs as task-constrained principle components and evaluating category membership in terms of model fit-that is, the fidelity of the reconstruction after recoding and decoding the stimulus. The approach is implemented as a computational model called DIVA (for DIVergent Autoencoder), an artificial neural network that uses reconstructive learning to solve N-way classification tasks. DIVA shows good qualitative fits to benchmark human learning data and provides a compelling theoretical alternative to established models.",
  journal   = "Psychon. Bull. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  14,
  number    =  4,
  pages     = "560--576",
  month     =  aug,
  year      =  2007,
  language  = "en"
}

@article{Schlegelmilch2018,
  title={CALM - A Process Model of Category Generalization, Abstraction and Structuring},
  author={Ren{\'e} Schlegelmilch and Andy J. Wills and Bettina von Helversen},
  journal={Cognitive Science},
  year={2018}
}

@ARTICLE{VanDoorn2021,
  title     = "The {JASP} guidelines for conducting and reporting a Bayesian analysis",
  author    = "van Doorn, Johnny and van den Bergh, Don and Böhm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Etz, Alexander and Evans, Nathan J and Gronau, Quentin F and Haaf, Julia M and Hinne, Max and Kucharsk{\'y}, {\v S}imon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Gupta, Akash R Komarlu Narendra and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G and Wagenmakers, Eric-Jan",
  abstract  = "Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed recommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared towards analyses performed with the open-source statistical software JASP, most guidelines extend to Bayesian inference in  general.",
  journal   = "Psychon. Bull. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  28,
  number    =  3,
  pages     = "813--826",
  month     =  jun,
  year      =  2021,
  keywords  = "Bayesian inference; Scientific reporting; Statistical software",
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@article{Nosofsky1994,
  title={Comparing modes of rule-based classification learning: A replication and extension of Shepard, Hovland, and Jenkins (1961)},
  author={Robert M. Nosofsky and Mark A. Gluck and Thomas J. Palmeri and Stephen C. McKinley and P. Glauthier},
  journal={Memory \& Cognition},
  year={1994},
  volume={22},
  pages={352-369}
}

@article{Nosofsky1986,
  title={Attention, similarity, and the identification-categorization relationship.},
  author={Robert M. Nosofsky},
  journal={Journal of experimental psychology. General},
  year={1986},
  volume={115 1},
  pages={39-61}
}

@article{Ashby1988,
  title={Decision rules in the perception and categorization of multidimensional stimuli.},
  author={F. Gregory Ashby and Ralph E. Gott},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={1988},
  volume={14 1},
  pages={33-53}
}

@article{Reed1972,
   title = {Pattern recognition and categorization},
   journal = {Cognitive Psychology},
   volume = {3},
   number = {3},
   pages = {382-407},
   year = {1972},
   issn = {0010-0285},
   doi = {https://doi.org/10.1016/0010-0285(72)90014-X},
   url = {https://www.sciencedirect.com/science/article/pii/001002857290014X},
   author = {Stephen K. Reed},
   abstract = {Four experiments are reported which attempt to determine how people make classifications when categories are defined by sets of exemplars and not by logical rules. College students classified schematic faces into one of two categories each composed of five faces. One probability model and three distance models were tested. The predominant strategy, as revealed by successful models, was to abstract a prototype representing each category and to compare the distance of novel patterns to each prototype, emphasizing those features which best discriminated the two categories.}
}

@article{Posner1968,
   abstract = {PREVIOUS WORK INDICATES THAT SS CAN LEARN TO CLASSIFY SETS OF PATTERNS WHICH ARE DISTORTIONS OF A PROTOTYPE THEY HAVE NOT SEEN. IT IS SHOWN THAT AFTER LEARNING A SET OF PATTERNS, THE PROTOTYPE (SCHEMA) OF THAT SET IS MORE EASILY CLASSIFIED THAN CONTROL PATTERNS ALSO WITHIN THE LEARNED CATEGORY. AS THE VARIABILITY AMONG THE MEMORIZED PATTERNS INCREASES, SO DOES THE ABILITY OF SS TO CLASSIFY HIGHLY DISTORTED NEW INSTANCES. THESE FINDINGS ARGUE THAT INFORMATION ABOUT THE SCHEMA IS ABSTRACTED FROM THE STORED INSTANCES WITH VERY HIGH EFFICIENCY. IT IS UNCLEAR WHETHER THE ABSTRACTION OF INFORMATION INVOLVED IN CLASSIFYING THE SCHEMA OCCURS WHILE LEARNING THE ORIGINAL PATTERNS OR WHETHER THE ABSTRACTION PROCESS OCCURS AT THE TIME OF THE 1ST PRESENTATION OF THE SCHEMA. (PsycINFO Database Record (c) 2006 APA, all rights reserved). 1968 American Psychological Association.},
   author = {Michael I. Posner and Steven W. Keele},
   doi = {10.1037/H0025953},
   issn = {00221015},
   issue = {3 PART 1},
   journal = {Journal of Experimental Psychology},
   keywords = {PATTERN DISTORTIONS, ABSTRACTION OF CRITERIA},
   month = {7},
   pages = {353-363},
   pmid = {5665566},
   title = {On the genesis of abstract ideas},
   volume = {77},
   url = {/record/1968-14980-001},
   year = {1968},
}

@article{Erickson1998,
   abstract = {Psychological theories of categorization generally focus on either rule- or exemplar-based explanations. We present 2 experiments that show evidence of both rule induction and exemplar encoding as well as a connectionist model, ATRIUM, that specifies a mechanism for combining rule- and exemplar-based representation. In 2 experiments participants learned to classify items, most of which followed a simple rule, although there were a few frequently occurring exceptions. Experiment 1 examined how people extrapolate beyond the range of training. Experiment 2 examined the effect of instance frequency on generalization. Categorization behavior was well described by the model, in which exemplar representation is used for both rule and exception processing. A key element in correctly modeling these results was capturing the interaction between the rule- and exemplar-based representations by using shifts of attention between rules and exemplars.},
   author = {Michael A. Erickson and John K. Kruschke},
   doi = {10.1037/0096-3445.127.2.107},
   issn = {00963445},
   issue = {2},
   journal = {Journal of Experimental Psychology: General},
   pages = {107-140},
   pmid = {9622910},
   publisher = {American Psychological Association Inc.},
   title = {Rules and exemplars in category learning.},
   volume = {127},
   year = {1998},
}

@article{Medin1978,
   abstract = {Most theories dealing with ill-defined concepts assume that performance is based on category level information or a mixture of category level and specific item information. A context theory of classification is described in which judgments are assumed to derive exclusively from stored exemplar information. The main idea is that a probe item acts as a retrieval cue to access information associated with stimuli similar to the probe. The predictions of the context theory are contrasted with those of a class of theories (including prototype theory) that assume that the information entering into judgments can be derived from an additive combination of information from component cue dimensions. Across 4 experiments with 128 paid Ss, using both geometric forms and schematic faces as stimuli, the context theory consistently gave a better account of the data. The relation of context theory to other theories and phenomena associated with ill-defined concepts is discussed in detail. (42 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). 1978 American Psychological Association.},
   author = {Douglas L. Medin and Marguerite M. Schaffer},
   doi = {10.1037/0033-295X.85.3.207},
   issn = {0033295X},
   issue = {3},
   journal = {Psychological Review},
   keywords = {test of context theory vs other theories of ill-defined concepts, classification learning, 17-30 yr olds},
   month = {5},
   pages = {207-238},
   title = {Context theory of classification learning},
   volume = {85},
   url = {/record/1979-12633-001},
   year = {1978},
}

@article{Kruschke2001,
   abstract = {The inverse base-rate effect is a phenomenon in which people learn about some common and some rare outcomes and in subsequent testing people predict the rare outcome for particular sets of conflicting cues, contrary to normative predictions. P. Juslin, P. Wennerholm, and A. Winman suggested that the effect could be explained by eliminative inference, contrary to the attention-shifting explanation of J. K. Kruschke. The present article shows that the eliminative inference model exhibits ordinal discrepancies from previously published data and from data of 2 new experiments. A connectionist implementation of attentional theory fits the data well. The author concludes that people can use eliminative inference but that it cannot account for the inverse base-rate effect.},
   author = {John K. Kruschke},
   doi = {10.1037//0278-7393.27.6.1385},
   issn = {0278-7393},
   issue = {6},
   journal = {Journal of experimental psychology. Learning, memory, and cognition},
   pages = {1385-1400},
   publisher = {American Psychological Association (APA)},
   title = {The inverse base-rate effect is not explained by eliminative inference.},
   volume = {27},
   year = {2001},
}

@article{Kruschke2003,
   abstract = {A. Winman, P. Wennerholm, and P. Juslin (2003) have admitted that J. K. Kruschke (2001a) cogently demonstrated the shortcomings of eliminative inference as an explanation of the inverse base rate effect, but they raise criticisms of Kruschke's attentionally based explanation. First, Winman et al. pointed out that attentional shifting does not improve learning performance in Kruschke's (1996) ADIT model, contrary to the claims that attentional shifting accelerates learning. This reply demonstrates that the deceleration of learning is a natural consequence when attentional shifts are not learned, as is the case in ADIT; however, when attentional shifts are learned, as was assumed by the underlying theory and as is the case in the EXIT model (Kruschke, 2001a, 2001b), then performance is indeed accelerated by attentional shifts. Second, Winman et al. pointed out that, whereas EXIT captures essentially all of the notable effects in the transfer data, it fails to capture a small effect [viz., p(CPC) p(RPR)]. This reply demonstrates that when this trend in the data is merely weighted more heavily in the model fitting, then the EXIT model accommodates it. EXIT accomplishes this by emphasizing base rate learning more strongly. Thus, the EXIT model, and attentional theory more generally, remains a viable explanation of the inverse base rate effect.},
   author = {John K. Kruschke},
   doi = {10.1037/0278-7393.29.6.1396},
   journal = {Journal of Experimental Psychology:Copyright 2003 by the American Psychological Association, Inc.Learning, Memory, and Cognition},
   title = {Attentional Theory Is a Viable Explanation of the Inverse Base Rate Effect: A Reply to Winman, Wennerholm, and Juslin (2003)},
   year = {2003},
}

@article{Smith2004,
   author = {J. David Smith and John Paul Minda and David A. Washburn},
   doi = {10.1037/0096-3445.133.3.398},
   issn = {1939-2222},
   issue = {3},
   journal = {Journal of Experimental Psychology: General},
   pages = {398-414},
   title = {Category Learning in Rhesus Monkeys: A Study of the Shepard, Hovland, and Jenkins (1961) Tasks.},
   volume = {133},
   year = {2004},
}