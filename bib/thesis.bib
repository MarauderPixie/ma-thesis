@article{Schad2021,
   abstract = {Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes factors provide one general way to compare different hypotheses by their compatibility with the observed data. Those quantifications can then also be used to choose between hypotheses. While Bayes factors provide an immediate approach to hypothesis testing, they are highly sensitive to details of the data/model assumptions. Moreover it's not clear how straightforwardly this approach can be implemented in practice, and in particular how sensitive it is to the details of the computational implementation. Here, we investigate these questions for Bayes factor analyses in the cognitive sciences. We explain the statistics underlying Bayes factors as a tool for Bayesian inferences and discuss that utility functions are needed for principled decisions on hypotheses. Next, we study how Bayes factors misbehave under different conditions. This includes a study of errors in the estimation of Bayes factors. Importantly, it is unknown whether Bayes factor estimates based on bridge sampling are unbiased for complex analyses. We are the first to use simulation-based calibration as a tool to test the accuracy of Bayes factor estimates. Moreover, we study how stable Bayes factors are against different MCMC draws. We moreover study how Bayes factors depend on variation in the data. We also look at variability of decisions based on Bayes factors and how to optimize decisions using a utility function. We outline a Bayes factor workflow that researchers can use to study whether Bayes factors are robust for their individual analysis, and we illustrate this workflow using an example from the cognitive sciences. We hope that this study will provide a workflow to test the strengths and limitations of Bayes factors as a way to quantify evidence in support of scientific hypotheses. Reproducible code is available from https://osf.io/y354c/.},
   author = {Daniel J. Schad and Bruno Nicenboim and Paul-Christian Bürkner and Michael Betancourt and Shravan Vasishth},
   month = {3},
   title = {Workflow Techniques for the Robust Use of Bayes Factors},
   url = {http://arxiv.org/abs/2103.08744},
   year = {2021},
}
@article{Chen2017,
   abstract = {Objective: In intervention research, the decision to continue developing a new program or treatment is dependent on both the change-inducing potential of a new strategy (i.e., its effect size) and the methods used to measure change, including the size of samples. This article describes a Bayesian approach to determining sample sizes in the sequential development of interventions. Description: Because sample sizes are related to the likelihood of detecting program effects, large samples are preferred. But in the design and development process that characterizes intervention research, smaller scale studies are usually required to justify more costly, larger scale studies. We present 4 scenarios designed to address common but complex questions regarding sample-size determination and the risk of observing misleading (e.g., false-positive) findings. From a Bayesian perspective, this article describes the use of decision rules composed of different target probabilities and prespecified effect sizes. Monte-Carlo simulations are used to demonstrate a Bayesian approach—which tends to require smaller samples than the classical frequentist approach—in the development of interventions from one study to the next.},
   author = {Ding Geng Chen and Mark W. Fraser},
   doi = {10.1086/693433},
   issn = {1948822X},
   issue = {3},
   journal = {Journal of the Society for Social Work and Research},
   keywords = {Bayesian,Intervention research,Monte-Carlo simulation,Research design,Sample size},
   month = {9},
   pages = {457-470},
   publisher = {University of Chicago Press},
   title = {A bayesian approach to sample size estimation and the decision to continue program development in intervention research},
   volume = {8},
   year = {2017},
}
@article{CK17,
   abstract = {Reference point approaches have dominated the study of categorization for decades by explaining classification learning in terms of similarity to stored exemplars or averages of exemplars. The most successful reference point models are firmly grounded in the associative learning tradition—treating categorization as a stimulus generalization process based on inverse exponential distance in psychological space augmented by a dimensional selective attention mechanism. We present experiments that pose a significant challenge to popular reference point accounts which explain categorization in terms of stimulus generalization from exemplars, prototypes, or adaptive clusters. DIVA, a similarity-based alternative to the reference point framework, provides a successful account of the human data. These findings suggest that a successful psychology of categorization may need to look beyond stimulus generalization and toward a view of category learning as the induction of a richer model of the data.},
   author = {Nolan Conaway and Kenneth J. Kurtz},
   doi = {10.3758/s13423-016-1208-1},
   issn = {15315320},
   issue = {4},
   journal = {Psychonomic Bulletin and Review},
   keywords = {Categorization,Classification learning,Concepts,Formal models,Generalization,Neural network models,Stimulus generalization theory},
   month = {12},
   pages = {1312-1323},
   pmid = {27981437},
   publisher = {Springer New York LLC},
   title = {Similar to the category, but not the exemplars: A study of generalization},
   volume = {24},
   year = {2017},
}
@report{Shepard1961,
   author = {Roger N Shepard and Carl I Hovland and Herbert M Jenkins},
   issue = {13},
   title = {Psychological Monographs: General and Applied LEARNING AND MEMORIZATION OF CLASSIFICATIONS1},
   volume = {75},
   year = {1961},
}
@article{Kurtz2013,
   abstract = {The findings of Shepard, Hovland, and Jenkins (1961) on the relative ease of learning 6 elemental types of 2-way classifications have been deeply influential 2 times over: 1st, as a rebuke to pure stimulus generalization accounts, and again as the leading benchmark for evaluating formal models of human category learning. The litmus test for models is the ability to simulate an observed advantage in learning a category structure based on an exclusive-or (XOR) rule over 2 relevant dimensions (Type 11) relative to category structures that have no perfectly predictive cue or cue combination (including the linearly-separable Type IV). However, a review of the literature reveals that a Type II advantage over Type IV is found only under highly specific experimental conditions. We investigate when and why a Type II advantage exists to determine the appropriate benchmark for models and the psychological theories they represent. A series of 8 experiments link particular conditions of learning to outcomes ranging from a traditional Type 11 advantage to compelling non-differences and reversals (i.e.. Type IV advantage). Common interpretations of the Type II advantage as either a broad-based phenomenon of human learning or as strong evidence for an attention-mediated similarity-based account are called into question by our findings. Finally, a role for verbalization in the category learning process is supported. The seminal research of Shepard, Hovland, and Jenkins (1961) shifted the direction of scientific thinking on concept learning when it first appeared. The work has more than stood the test of time-50 years later the most important benchmark for evaluating formal accounts of classification learning is the observed ordering of the ease of learning of the SHJ (for short) elemental category},
   author = {Kenneth J Kurtz and Kimery R Levering and Roger D Stanton and Joshua Romero and Steven N Morris and J Kurtz},
   doi = {10.1037/a0029I78},
   issue = {2},
   journal = {Mathy \& Brad-metz},
   keywords = {category learning,formal models of category learning,selective attention,task ^instructions,verbalization},
   pages = {552-572},
   publisher = {Pitt},
   title = {Human Learning of Elemental Category Structures: Revisiting the Classic Result of Shepard, Hovland, and Jenkins (1961)},
   volume = {39},
   year = {2013},
}
@article{Barr2013,
   abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
   author = {Dale J Barr and Roger Levy and Christoph Scheepers and Harry J Tily},
   doi = {https://doi.org/10.1016/j.jml.2012.11.001},
   issn = {0749-596X},
   issue = {3},
   journal = {Journal of Memory and Language},
   keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
   pages = {255-278},
   title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
   volume = {68},
   url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
   year = {2013},
}
@article{Andras2014,
   abstract = {In management research, empirical data are often analyzed using p-value null hypothesis significance testing (pNHST). Here we outline the conceptual and practical advantages of an alternative analysis method: Bayesian hypothesis testing and model selection using the Bayes factor. In contrast to pNHST, Bayes factors allow researchers to quantify evidence in favor of the null hypothesis. Also, Bayes factors do not require adjustment for the intention with which the data were collected. The use of Bayes factors is demonstrated through an extended example for hierarchical regression based on the design of an experiment recently published in the Journal of Management. This example also highlights the fact that p values overestimate the evidence against the null hypothesis, misleading researchers into believing that their findings are more reliable than is warranted by the data.},
   author = {Sandra Andraszewicz and Benjamin Scheibehenne and Jörg Rieskamp and Raoul Grasman and Josine Verhagen and Eric-Jan Wagenmakers},
   doi = {10.1177/0149206314560412},
   issn = {0149-2063},
   issue = {2},
   journal = {Journal of Management},
   month = {12},
   note = {doi: 10.1177/0149206314560412},
   pages = {521-543},
   publisher = {SAGE Publications Inc},
   title = {An Introduction to Bayesian Hypothesis Testing for Management Research},
   volume = {41},
   url = {https://doi.org/10.1177/0149206314560412},
   year = {2014},
}
@article{Gronau2020,
   abstract = {Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve highdimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.},
   author = {Quentin F Gronau and Henrik Singmann and Eric-Jan Wagenmakers},
   doi = {10.18637/jss.v092.i10},
   issue = {10},
   journal = {Journal of Statistical Software},
   month = {2},
   pages = {1 - 29},
   title = {bridgesampling: An R Package for Estimating Normalizing Constants},
   volume = {92},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v092i10},
   year = {2020},
}
@article{Schlegelmilch2021,
   author = {René Schlegelmilch and Andy J. Wills and Bettina von Helversen},
   doi = {10.1037/rev0000321},
   issn = {1939-1471},
   journal = {Psychological Review},
   month = {9},
   title = {A cognitive category-learning model of rule abstraction, attention learning, and contextual modulation.},
   year = {2021},
}
@article{Schlegelmilch2020,
   author = {René Schlegelmilch and Bettina von Helversen},
   doi = {10.1037/xge0000747},
   issn = {1939-2222},
   issue = {10},
   journal = {Journal of Experimental Psychology: General},
   month = {10},
   pages = {1823-1854},
   title = {The influence of reward magnitude on stimulus memory and stimulus generalization in categorization decisions.},
   volume = {149},
   year = {2020},
}
