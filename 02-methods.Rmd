---
chapter: 2
knit: "bookdown::render_book"
---

```{r demo_data}
dur <- quantile(demo$duration, c(.025, .95)) |> as.numeric()
exc_id <- unique(exclud_trans$subj_id)
```

# Methods {#ch:methods}

An online experiment has been conducted using the \_magpie 3 framework (https://magpie-ea.github.io/magpie-site/). This chapter describes the collected sample, the experimental setup and procedure, the data preparation and the statistical methods used for data analysis. The experiment has been pre-registered according to the guidelines of the Open Science Foundation (https://osf.io/)^[Pre-registration: [https://osf.io/wegm9](https://osf.io/wegm9)]. All code regarding the experiment and the data analysis can be found on the corresponding project page^[Project: [https://osf.io/gup7x/](https://osf.io/gup7x/)].

## Data collection procedure and sample description

For data collection, the mailing lists of the psychology and cognitive science courses of the University of Osnabrück and the psychology mailing list of the University of Bremen have been used, additional to several social media channels like Facebook groups for psychology students and verbal recruitment. Participation was completely anonymous, voluntary and could be withdrawn at any time before submitting results. Students of said courses had the opportunity to receive 0.5 participation hours as compensation^[In order to keep the experimental data anonymous, the necessary personal data needed to award VP-hours was collected in a seperate dataset which could not be traced back to the experimental dataset. For the same reason, participation could not be revoked after data has been submitted.] and participants were required to be at least 18 years old and native German speakers. 

A total of `r nrow(demo) + length(exc_id)` participants took part in the experiment, of which `r nrow(demo)` are eligible after exclusion. Average age of participants is `r mean(demo$age, na.rm = TRUE) |> round(1)` years (_min_ = `r min(demo$age, na.rm = TRUE)`, _max_ = `r max(demo$age, na.rm = TRUE)`, _SD_ = `r sd(demo$age, na.rm = TRUE) |> round(2)`) and median duration of participation is `r median(demo$duration, na.rm = TRUE) |> round()` seconds. There are `r demo %>% filter(duration > dur[2]) %>% nrow()` participants whose participation took more than 30 minutes (>95% quantile); without these, standard deviation is `r demo %>% filter(duration < dur[2]) %>% pull(duration) %>% sd() |> round()` seconds.


## Experimental setup and procedure

The basic experimental setup is an iteration of the incomplete-XOR task by @CK17, using the same set of stimuli (see figure \@ref(fig:ch2-stimuli)) and consisting of a training and a transfer phase. In the training phase of the experiment, participants were shown 6 squares of differing shade and size in a trial-and-feedback procedure. Upon presentation, they had to predict whether the given stimulus belongs to fictionally labeled categories "Nobz" or "Grot". After making their choice, participants received feedback on whether they were correct. The stimuli of the incomplete category were displayed twice per block to "equate for category frequency" [@CK17], resulting in 8 trials per block across all conditions; all stimuli in the first 2 blocks / 16 trials of the blocked condition have also been shown twice per block. The training phase consists of 12 randomized blocks and therefore 96 trials in total.

(ref:ch2-stimuli) (A) Full set of stimuli used during the transfer phases, taken from @CK17. During the experiment, only one square was shown at a time. (B) Subset of stimuli used during training and unobserved subset of critical stimuli for extrapolation (marked X).

```{r ch2-stimuli, fig.show="hold", out.height="33%", out.width="100%", fig.cap="(ref:ch2-stimuli)"}
# knitr::include_graphics(c("figures/all-stimuli-arranged.png", 
#                           "figures/stimuli-trainfer.png"))
knitr::include_graphics("figures/stimulus-grids.png")
```

In a subsequent transfer phase, the full set of 49 stimuli had to be classified without feedback, but otherwise in the same manner. Following that is a second transfer phase, although the second time participants were instructed to make a probability judgement about stimulus category membership (e.g. 30% Nobz, 70% Grot) via a slider rating ranging from 0% to 100% in steps of 1%.

At the end of the experiment, pariticipants are asked to fill out basic demographic information and to answer a question about their strategy during transfer. A last question asked participants whether they have any reason to believe their data should not be used, like distractions, noise or inattentivenes or. All answers to these questions were given voluntarily and could have been left blank. 

### Experimental manipulation {#ch2-setup}

In addition to the basic setup of @CK17 there are four different experimental groups: The first group serves as control group that receivs no experimental treatment, meaning they aren't told to look for rules and encounter all training stimuli intermixed right from the beginning. To address hypothesis 1.1, the "Blocked Rules" group sequentially learns two rules: in the first two training blocks they learn to classify stimuli along one dimension only, e.g. they only encounter large squares after training block two (see figure \@ref(fig:ch2-blocked)). For hypothesis 1.2 a "Rule Instrucions" group encounters all training stimuli from the beginning but is explicitly told twice in their task instructions to look for a rule to classify the stimuli by. This group's learning phase can also be compared against the first in order to approach hyppthesis 2. A fourth "Blocked + Instructions" group is subject to both manipulations by which a possible (additive) interaction can be investigated with regard to hypothesis 1.3. 

This setup results in a two-factorial between subjects design with two levels each: instructions (rule vs. neutral) and training order (blocked vs. mixed). The relevant part of the instructions **without mentioning rules:** reads: "[You'll be shown] geometric shapes that differ in color and size. Your task is to learn, whether a figure belongs to category **Nobz** or category **Grot** [...] Do your best to master the categories Nobz and Grot!" The ones that **do mention rules** on the other hand read: "Your task is to learn a rule on the basis of color and size by which you can decide whether a figure belongs to category **Nobz** or category **Grot** [...] Do your best to master the rule for categories Nobz and Grot!"^[although the study was conducted in German, the translation is rather direct; see \@ref(apx-instructions) for the original and full instructions.] As mentioned before, a **blocked** rule means that stimuli in the first two blocks of training (16 trials) can be correctly classifed entirely bei either one of their dimensions. Only afterwards the task becomes more complicated by showing the two additional stimuli of the whole training set (see fig. \@ref(fig:ch2-stimuli) for clarification). The **mixed** rule simply shows all training trials (and thereby the whole category structure) at random from the beginning (see fig. \@ref(fig:ch2-blocked)).

(ref:ch2-blocked) Example of a blocked and a mixed training block. The mixed set contains all available training stimuli, while the blocked set contains only stimuli that can be classified along one dimension (size in this case).

```{r ch2-blocked, fig.show="hold", out.height="16%", out.width="100%", fig.cap="(ref:ch2-blocked)"}
# knitr::include_graphics(c("figures/all-stimuli-arranged.png", 
#                           "figures/stimuli-trainfer.png"))
knitr::include_graphics("figures/mixblock.png")
```

### Randomization

Participants are randomly assigned to experimental groups by _magpie3's balanced randomization functionality upon opening the link to the experiment. For the training phase, the labels of categories (which set of stimuli corresponds to Nobz and which to Grot) is randomized as well as the dimension (size / shade) by which the stimuli of the blocked rule condition can be classified by. Trial randomization happens on a by-block level. For the transfer phases, the full set of stimuli is randomized blockwise as a whole. 


## Data preparation and exclusion

A sample size of 40 complete and eligible observations (after exclusion) per experimental group was preregistered, resulting in a total of 160 participations required. A power calculation for a simple logistic regression assuming an odds ratio of 2, requires 34 people per group to reach a power of 80%. @Chen2010 show that this assumption translates to a rather small effect, with an OR of 1.68 being equivalent to a Cohen‘s d of 0.2. Simulations by @Chen2017 show similar results and @Brysbaert2018 also recommend a sample size of roughly 40 people (with equally as many stimuli), albeit on a more general basis. 

### Data exclusion 

Participants that appear to be guessing in the training phase (the probability of correct classification on a single trial being 0.5) are excluded. The above-chance number of correct classifications needed (based on a 5% alpha level) in the last 3 training blocks (24 trials) then is 16. Additionally, any participant answering the question about any reason to not use their data in the affirmative is excluded as well - although in practice no participant did actually indicate this.

While two people were identified by the criterion above, after data collection and upon closer inspection, three additional participants have been identified that didn't learn the category strucure. Two participants classified stimuli exclusively along one stimulus dimension only, which can lead to 18 correctly classified stimuli and 6 errors, thereby passing the threshold. A fifth participant barely passed but did not seem to give reasonable responses in the transfer phase. Figure \@ref(fig:ch2-plot-exclusions) shows the accuracy of the last three training blocks and the responses in the transfer phase of the excluded individuals.

(ref:ch2-plot-exclusions) Response accuracy for excluded participants on the last 3 blocks of training and respective responses of the transfer phase. Each square maps 1:1 to the stimulus positions as shown in fig. \@ref(fig:ch2-stimuli).

```{r ch2-plot-exclusions, fig.cap="(ref:ch2-plot-exclusions)", fig.height=2.2}
plex_train <- exclud_train %>% 
  left_join(distinct(select(exclud_trans, image, img_x, img_y)), by = "image") %>% 
  filter(block > 9) %>% 
  group_by(subj_id, img_x, img_y) %>% 
  summarise(
    p_correct = round(mean(correct) * 100, 1),
    img_x = ifelse(img_x > 2, img_x - 2, img_x),
    img_y = ifelse(img_y > 2, img_y - 2, img_y),
    .groups = "drop"
  ) %>%
  ggplot(aes(img_x, img_y, fill = p_correct)) +
    facet_wrap(~subj_id, nrow = 1) +
    geom_tile(size = .5) +
    scale_fill_viridis_c(
      guide = guide_colorbar(
        barwidth = 1,
        barheight = 3
      )
    ) +
    labs(fill = "Acc.") +
    theme_subjects +
    theme(panel.border = element_rect(color = "#3c3c3c", fill = NA),
          legend.text  = element_text(size = 6),
          legend.position = "right",
          legend.justification = c(.9, .1))

plex_trans <- ggplot(exclud_trans, aes(img_x, img_y, fill = response)) +
  facet_wrap(~subj_id, nrow = 1) +
  geom_tile(size = .5, color = "white") +
  scale_fill_manual(values = c("#ac0634", "#525252")) +
  theme_subjects +
  theme(panel.border = element_rect(color = "#525252", fill = NA),
        legend.text  = element_text(size = 6),
        legend.position = "none")

plex_train / plex_trans
```


## Analysis plan

All analyses are conducted using R [@R4, v`r paste0(R.Version()$major, ".", R.Version()$minor, " - ", R.Version()$nickname)`]. All raw and prepared data as well as the corresponding analysis scripts are available on the osf repository^[Again, the link to the project for convenience: [https://osf.io/gup7x/](https://osf.io/gup7x/)]. 

To investigate the hypotheses laid out in section \@ref(01-hypotheses), several bayesian mixed-effect logistic regression models using a logit link function are fit with weakly informative priors. @Gelman2020 recommends using $\mathrm{student}(\mathrm{df}, \mathrm{location} = 0, \mathrm{scale} = 1)$ where $3 \leq df \leq 7$, since the often used Cauchy-Distribution (that is, $\mathrm{student}(1, 0, 2.5)$) has too thick tails and a strict Gaussian may be too informative. Finally, Bayes Factors for model comparisons are obtained by computing marginal likelihooods via bridge sampling [@Gronau2020]. Since that method requries a large number of posterior samples to obtain robust bayes factors, each model will have 4 MCMC chains with 12000 samples and 2000 warm-up samples each, resulting in 40000 posterior samples drawn per model [for details, see @Schad2021]. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 [@Vehtari2021], and Effective Sample Size (ESS), which should be greater than 1000 [@Buerkner2017]. @VanDoorn2021 recommend conducing a bayes factor robustness check by comparing the same models using different priors. This has been done for the model comparisons of H1.1, H1.2 and H2, the results of which can be found in appendix \@ref(bfrc).

### Model comparisons in detail

**Effects on extrapolations --** For hypotheses 1, only the trials of the nine critical items of the incomplete category of the transfer are used (marked X in fig. \@ref(fig:ch2-stimuli)) for analysis. Both H1.1 and H1.2 assume one main effect each, namely one of rule instructions and one of blocked training; H1.3 assumes an interaction effect of both. Therefore, five models have been fitted: (1) a full model containing all effects and their interaction, (2) a reduced model with both main effects but no interaction term, (3) a reduced model including just the effect of instructions, (4) one including just the blocked rule effect and finally (5) a reduced model that included no effects at all; also, all models except (1) don't include an interaction term. The computation of bayes factors can be approached by two ways, either additively or subtractively. An additive approach means to sequentially add effects to a model and comparing it to the same model without the effect. A substractive approach takes the other direction and sequentially removes effects, comparing the smaller to the larger. Since in this case only two main effects are considered, both approaches are equal. Bayes factors are therefore computed for each model compared to model (5), such that the results can be read as "extrapolations are x times more/less likely to occur when exposed to a blocked training condition than when not".

**Effect on Learning --** In a similar fashion, hypothesis 2 is tested on a subset of data including only the groups receiving rule instructions and no treatment. In this case, there are only two models: One that includes the main effect of rule instructions and one that includes no effect, which again are compared via bayes factor.

All model specifications, including the choice of priors, are outlined in detail in their respective results section.

### Inference criteria {#02-ic}

Hypotheses are evaluated with Bayes Factors according to @Andras2014, adjusted from Jeffreys (1961): A $BF_{12} > 3$ constitutes moderate evidence for a model $M_1$ over a model $M_2$ (or vice versa when $BF_{12} < 1/3$) and $BF_{12} > 10$ (or $BF_{12} < 1/10$) will be interpreted as strong evidence respectively. Bayes Factors between 1/3 and 3 are considered to indicate no meaningful evidence in favor of either model.

### Exploration

The second transfer phase in which participants are asked about their probability judgement for any given stimulus belonging to either category is not linked to a specific hypotheses. This study looks at these in an exploratory fashion. For example, probability judgements allow a look at decision gradients on an individual level or whether stimuli near the center of the grid are classified with as much certainty as the ones in the corners (with regard to the visualization in fig. \@ref(fig:ch2-stimuli)). In terms of feature similarity that is to say, stimuli that are closer to members of other categories than to their most distinctive class members.