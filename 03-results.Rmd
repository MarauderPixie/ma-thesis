---
chapter: 3
knit: "bookdown::render_book"
---

```{r ch3-models-and-bayes-factors, cache=TRUE}
h1_null  <- readRDS("models/h1_transfer/h10.rds")
h1_block <- readRDS("models/h1_transfer/h11.rds")
h1_instr <- readRDS("models/h1_transfer/h12.rds")
h1_both  <- readRDS("models/h1_transfer/h131.rds")
h1_inter <- readRDS("models/h1_transfer/h132.rds")

# h2_null  <- readRDS("models/h2_training/h2_null.rds")
# h2_rules <- readRDS("models/h2_training/h2_rules.rds")
# h2_inter <- readRDS("models/h2_training/h2_inter.rds")

# bf_h11  <- bayes_factor(h1_blocked, h1_null)
# bf_h12  <- bayes_factor(h1_instructions, h1_null)
# bf_h131 <- bayes_factor(h1_both, h1_blocked)
# bf_h132 <- bayes_factor(h1_both, h1_instructions)
# bf_h133 <- bayes_factor(h1_both, h1_null)

# bf_h21  <- bayes_factor(h2_rules, h2_null)
# bf_h221 <- bayes_factor(h2_interaction, h2_rules)
# bf_h222 <- bayes_factor(h2_interaction, h2_null)
```

# Results {#ch:results}

## H1: Extrapolations in Transfer Phase

```{r h1-tbl-summary}
beobachtet <- extra_binom %>% 
  group_by(rules, blocked) %>% 
  summarise(
    mean_p = mean(p),
    sd_p   = sd(p),
    mean_k = mean(k),
    sd_k   = sd(k),
    mean_ckab6 = mean(exab6),
    mean_ckab5 = mean(exab5),
    .groups = "drop"
  )

beobachtet %>% 
    transmute(
        Group = case_when(
            rules == "no" & blocked == "no" ~ "No Treatment",
            rules == "no" & blocked == "yes" ~ "Blocked Rules",
            rules == "yes" & blocked == "no" ~ "Rule Instructions",
            rules == "yes" & blocked == "yes" ~ "Blocked + Instructions",
        ),
        "Mean (SD)" = paste0(round(mean_p * 100, 2), "% (", round(sd_p * 100, 2), ")"),
        "Extrapolators"   = paste0(round(mean_ckab6 * 100), "%")
    ) %>% 
  kbl(booktabs = TRUE, caption = "Mean proportion of extrapolated stimuli per person and proportion of extrapolators (k>6)") %>%
  kable_styling(full_width = FALSE, position = "left")
```

```{r h1-plots-hist, fig.cap="Number of extrapolated stimuli per person and condition"}
transfer %>% 
  filter(item == "transfer") %>% 
  group_by(subj_id, condition) %>% 
  summarise(ext = sum(extrapolation)) %>% 
  ggplot(aes(ext)) +
    facet_wrap(~condition) +
    geom_histogram(binwidth = 1) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), labels = c("", 0:9, "")) +
    labs(x = "Extrapolations", y = "people")
```

The main interest of this study was to find out whether it is possible to increase the number of extrapolations and thereby the learning of a full XOR category structure via the use of rule instructions, the ordering of rules to learn and to look into a possible interaction of both. Table \@ref(tab:h1-tbl-summary) shows the mean percentage of extrapolated stimuli per person by experimental condition as well as the percentage of extrapolators, that is people who extrapolated more than six of the nine critical stimuli. Figure \@ref(fig:h1-plots-hist) shows the number of extrapolations per person overall as well as between experimental groups. Figure \@ref(fig:h1-plots-gradient) on the other hand shows decision gradients for all stimuli across the conditions^[see fig. \@ref(fig:appA-transfer-plot) in appendix for an overview of individual responses.].


```{r h1-plots-gradient, fig.asp=1/1, fig.cap="Gradient of category decisions in the transfer phase"}
transfer %>% 
  count(condition, img_x, img_y, response) %>% 
  spread(response, n) %>% 
  mutate(
    # extra = ifelse(subj_id %in% extra_pols$subj_id, "Extrapolators", "Proximators"),
    Nobz = ifelse(is.na(Nobz), 0, Nobz),
    Grot = ifelse(is.na(Grot), 0, Grot),
    n = Nobz + Grot,
    p_Grot = (Grot / n * 100) |> round(2)
  ) %>% 
  ggplot(aes(img_x, img_y, fill = p_Grot)) +
    facet_wrap(~condition) +
    geom_tile(size = 1) +
    scale_fill_viridis_c() +
    theme_transfer +
    theme(legend.position = "none")
```

## Model comparisons

In order to quantify evidence for or against an effect of rule structuring, we fitted several Bayesian logistic models (estimated using MCMC sampling with 4 chains of 12000 iterations and a warmup of 2000) to predict the number of extrapolated stimulus responses per person. Priors over parameters for all models were set as:

- $student\_t(df = 3, location = -1, scale = 1)$ for the intercept, assuming about 30% extrapolators as reported by @CK17
- $student\_t(df = 3, location = 0.5, scale = 1)$ for all slopes, assuming an approximately 7% increase in extrapolations in any group over the one receiving no treatment.

The final model was fit on aggregated data with fixed effects for the blocked rule condition, the rule instructions condition and their interaction. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 [@Vehtari2021], and Effective Sample Size (ESS), which should be greater than 1000 [@Buerkner2017], which again can be confirmed or all parameters. Bayes Factors for all model comparisons were obtained via bridge sampling and are shown in table \@ref(tab:h1-tbl-bfs). 

```{r h1-tbl-bfs}
readr::read_csv("models/BFs.csv") %>% 
  kbl(booktabs = TRUE, caption = "Bayes Factors of Model Comparisons") %>% 
  kable_styling(full_width = TRUE) %>% 
  add_header_above(c(" ", "BF of left hand model over:" = 4)) %>% 
  column_spec(1, width = "5cm")
```



## H2: Learning with Rule Instructions

A similar approach as under Section 1.1 was taken to investigate a possible advantageous effect of instructions mentioning rules on learning the category structure. 
