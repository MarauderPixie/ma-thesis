---
chapter: 3
knit: "bookdown::render_book"
---

```{r ch3-models-and-bayes-factors, cache=TRUE}
# h1_null  <- readRDS("models/h1_transfer/h10.rds")
# h1_block <- readRDS("models/h1_transfer/h11.rds")
# h1_instr <- readRDS("models/h1_transfer/h12.rds")
# h1_both  <- readRDS("models/h1_transfer/h131.rds")
# h1_inter <- readRDS("models/h1_transfer/h132.rds")
```

# Results {#ch:results}

The following chapter lays out the results of the experiment and the analyses. Beginning with a look at the descriptives we will deal with the hypotheses and close with some exploration. 

This chapter reports the results of the conducted analyses; first by looking at the data descriptively, then at the hypotheses and at last at some exploration.

`[maybe something about model- and prior selection?]`

## H1: Extrapolations in Transfer Phase

(ref:h1-tbl-summary) Mean proportion of extrapolated stimuli per person and proportion of extrapolators between experimental conditions.

```{r h1-tbl-summary}
beobachtet <- extra_binom %>% 
  group_by(Group, rules, blocked) %>% 
  summarise(
    n = n(),
    mean_p = mean(p),
    sd_p   = sd(p),
    mean_k = mean(k),
    sd_k   = sd(k),
    mean_ckab6 = mean(exab6),
    mean_ckab5 = mean(exab5),
    .groups = "drop"
  )

beobachtet %>% 
    transmute(
        Group = Group,
        "n" = n,
        "Extrapolators"   = paste0(round(mean_ckab6 * 100), "%"),
        "Mean" = paste0(round(mean_p * 100, 2), "%"),
        "SD"   = round(sd_p * 100, 2)
    ) %>% 
  # xtable(booktabs = TRUE, caption = "(ref:h1-tbl-summary)") %>%
  # xtable2kable() %>%
  kbl(booktabs = TRUE, caption = "(ref:h1-tbl-summary)") %>%
  kable_styling(full_width = FALSE, font_size = 10, 
                position = "center", latex_options = "hold_position") %>% # "float_right") %>% 
  add_header_above(c(" " = 3, "XOR Extrapolations" = 2))
```

The main interest of this study is to find out whether an increase in the number of XOR extrapolations and thereby the learning of a full XOR category structure is facilitated by the use of rule instructions, the ordering of rules during learning and a possible interaction of both. Table \@ref(tab:h1-tbl-summary) shows the final group sizes, the average percentage of extrapolated stimuli per person as well as the percentage of extrapolators - people who extrapolated at least 6 of the nine critical stimuli - between experimental conditions. Figure \@ref(fig:h1-plots-hist) shows the number of extrapolations per person overall as well as between experimental groups. Figure \@ref(fig:h1-plots-gradient) on the other hand shows decision gradients for all stimuli across the conditions^[see fig. \@ref(fig:apx-transfer) in the appendix for an overview of responses by every individual.]. `[may combine them into one plot]`

```{r h1-plots-hist, fig.cap="Number of extrapolated stimuli per person and condition", fig.asp=1/3}
transfer %>% 
  filter(item == "transfer") %>% 
  group_by(subj_id, Group) %>% 
  summarise(ext = sum(extrapolation)) %>%
  ggplot(aes(ext)) +
    facet_wrap(~Group, nrow = 1, 
               labeller = label_wrap_gen(width=10)) +
    geom_histogram(binwidth = 1) +
    scale_x_continuous(breaks = scales::pretty_breaks(n = 10), 
                       labels = c("", 0:9, "")) +
    labs(x = "Extrapolations", y = "n") +
    theme(panel.grid.minor.x = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.spacing = unit(5, "pt"))
```

```{r h1-plots-gradient, fig.cap="Gradient of category decisions in the transfer phase", fig.asp=1/2}
transfer %>% 
  left_join(proxy_ex_ids, by = c("subj_id", "Group")) %>% 
  count(Group, img_x, img_y, extrap, response) %>% 
  spread(response, n) %>% 
  mutate(
    Nobz = ifelse(is.na(Nobz), 0, Nobz),
    Grot = ifelse(is.na(Grot), 0, Grot),
    n = Nobz + Grot,
    p_Grot = (Grot / n * 100) |> round(2)
  ) %>% 
  ggplot(aes(img_x, img_y, fill = p_Grot)) +
    facet_grid(extrap~Group, switch = "y",
               labeller = label_wrap_gen(width=10)) +
    geom_tile(size = .2, color = "white") +
    scale_fill_viridis_c() +
    theme_transfer +
    theme(legend.position = "none",
          panel.spacing = unit(5, "pt"))
```

In order to quantify evidence for an effect of rule instructions and rule order, we fitted several Bayesian logistic models (estimated using MCMC sampling with 4 chains of 12000 iterations and a warmup of 2000) to predict the extrapolation of stimulus responses. Priors over parameters for all models were set as:

- $student\_t(df = 3, location = -1.4, scale = 1)$ for the intercept, assuming about 30% of the participants extrapolating at least 6 out ouf 9 trials as reported by @CK17
- $student\_t(df = 3, location = 0.5, scale = 1)$ for all slopes, assuming an approximately 10% increase in extrapolations in any group over the one receiving no treatment.
- $student\_t(df = 3, location = 2.2, scale = 1)$ with a lower bound of 0 and an upper bound of 5 for random intercepts per participant

The final model was fit with fixed effects for the blocked rule condition, the rule instructions condition and their interaction, as well as random intercepts per participant. Convergence and stability of the Bayesian sampling has been assessed using R-hat, which should be below 1.01 [@Vehtari2021], and Effective Sample Size (ESS), which should be greater than 1000 [@Buerkner2017], which in turn can be confirmed for all parameters of all models. Bayes Factors for all model comparisons were obtained by comparing marginal likelihoods via bridge sampling and are shown in table \@ref(tab:h1-tbl-bfs).
Compared to the intercept only model, we found moderate evidence (_BF_ = `r h1_bfs[1,2]`) in favour of the Blockd Rule model; no evidence (_BF_ = `r h1_bfs[2,2]`) against the Rule Instructions model (the least supported model); moderate evidence (_BF_ = `r h1_bfs[3,2]`) in favour of the model including both terms (the most supported model) and moderate evidence (_BF_ = `r h1_bfs[4,2]`) in favour of the model including both terms and their interaction. Since the model including both terms but no interaction is the most supported one, with an substantial explanatory power of $R^2 = 0.67$ (_CI_ = [0.64, 0.69]), that models posterior estimates are reported in table \@ref(tab:model-summary). 

(ref:h1-tbl-bfs) Model comparison for effects of rule instruction, blocked rules and their interaction; Bayes Factors for $M_1$ over $M_2$ obtained via bridge sampling.

```{r h1-tbl-bfs}
h1_bfs %>% 
  kbl(booktabs = TRUE, escape = FALSE, 
      caption = "(ref:h1-tbl-bfs)", align = "l") %>% 
  kable_styling(full_width = TRUE) %>% 
  add_header_above(c(" ", "$M_2$" = 4), escape = FALSE) %>% 
  column_spec(1, width = "5cm")
```

Table: (\#tab:model-summary) Ugly and absolutely unformatted, incomplete and rather convoluted model summary table.

Parameter      Median          95% CI     Rhat      ESS               95% CI       Fit
-------------  ------  ---------------   -----  -------  -------------------  --------
(Intercept)     -5.67  [-7.13, -4.42]    1.001  9427.00       [-7.14, -4.40]          
blockedyes       1.48  [ 0.28,  2.88]    1.001  7064.00       [ 0.26,  2.86]          
rulesyes         0.75  [-0.40,  1.97]    1.001  8583.00       [-0.39,  1.97]          
ELPD                                                                           -316.67
LOOIC                                                                           633.34
WAIC                                                                            613.95
R2                                                                                0.66
Sigma                                                                             1.81
Log_loss                                                                          0.14


## H2: Learning Accuracy with Rule Instructions

A similar approach as under Section 1.1 was taken to investigate a possible advantageous effect of instructions mentioning rules on learning the category structure. Two Bayesian 
logistic mixed models were fitted (estimated using MCMC sampling with 4 chains of 12000 iterations and a warmup of 2000) to predict correct classification of training stimuli. Priors over parameters were set as:

- $student\_t(df = 3, location = 2, scale = 1)$ for the intercept, assuming an average 88% learning accuracy
- $student\_t(df = 3, location = 0, scale = 1)$ for the slope of the rule instructions condition, assuming no difference.
- $student\_t(df = 3, location = 2.2, scale = 1)$ with a lower bound of 0 and an upper bound of 5 for all random effects

The first (intercept-only) model assumes random intercepts for participants with a by-participant random effect for training blocks. The second model assumes the same but adds the rule instructions condition as a fixed effect. Convergence and stability of the Bayesian sampling has again been assessed using R-hat and Effective Sample Size (ESS), which again can be confirmed for all parameters. Model comparison via bridge sampling yields no meaningful evidence against an effect of Rule Instructions ($BF_{21}$ = .565), even though the rule instructions model has substantial explanatory power ($R^2$ = 0.48, _CI_ = [0.43, 0.53]) and the rule effect has a 98.22% probability of being positive (_Median_ = 0.40, _95% HDI_= [0.03, 0.77]). 

`[here, a plot showing the learning "curves" would do nicely]`

## Exploration

`[decide on what to keep, what to say about it and whether to have some comparisons between "extrapolators" and "proximators"]`

```{r straight-outta-fibbleburg, out.width="80%"}
transp <- stimprob %>%
  left_join(proxy_ex_ids, by = c("subj_id", "Group")) 

transp %>% 
  group_by(Group, extrap, img_x, img_y) %>% 
  summarise(
    p = mean(prob),
    .groups = "drop"
  ) %>%  
  ggplot(aes(img_x, img_y, fill = p)) +
    facet_grid(extrap ~ Group, switch = "y",
               labeller = label_wrap_gen(width=10)) +
    # facet_wrap(~Group, nrow = 1) ~
    geom_tile(size = .5, color = "white") +
    scale_fill_viridis_c() +
    theme_transfer +
    theme(legend.position = "right",
          strip.text = element_text(size = 8),
          panel.spacing = unit(5, "pt"))
```

```{r auch-cool, fig.asp=1/1, caption="Pretty cool plot showing the distribution of probability judgements per stimulus that's sadly not all too informative"}
transp %>%
  ggplot(aes(x = prob)) +
    facet_grid(cols = vars(img_x), 
               rows = vars(7 - img_y)) +
    geom_histogram(binwidth = 10) +
    theme(strip.text  = element_blank(),
          axis.text.y = element_blank(),
          axis.text.x = element_blank(),
          panel.spacing = unit(5, "pt"),
          axis.title.x  = element_blank(),
          axis.title.y  = element_blank(),
          panel.grid.minor   = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.border = element_rect(color = "#3c3c3c", fill = NA))
```