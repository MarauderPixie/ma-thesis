---
chapter: 4
knit: "bookdown::render_book"
---

# Discussion

The goal of this study was to answer questions resulting from two key assumptions of the Category Abstraction Learning model (CAL) by @Schlegelmilch2021: (1) Can the process of contextual modulation and rule abstraction be facilitated by either establishing a simple rule before introducing a more complicated rule or (2) by explicitly prompting participants to find and apply a rule. An experiment was conducted using an incomplete-XOR task in which a total of 192 participants had to classify a set of stimuli they had not learned or encountered in a previous learning phase. This allowed to test the specific hypothesis that either rule instructions or a blocked training phase leads to more XOR-extrapolations and stronger XOR-learning. While there is moderate evidence for an effect of blocked training, an effect of rule instructions can not be confirmed. Furthermore, the proportion of people extrapolating the untrained stimuli to a full XOR solution is considerably lower than previously reported. While these results regaring the instructions can be explained within the CAL framework to some extent, the question of why there are so few extrapolators in this sample largely remains open. The following paragraphs take a closer look at the details.

As predicted by hypothesis 1.1, this study found moderate evidence for a main effect of a blocked rule on the number of extrapolations in the experimental transfer phase. 
Since spontanouos extrapolation of previously unencountered stimuli can not be explained well by classical reference point views on category learning [@Nosofsky1986; @Kruschke1992], this puts some weight to the assumption of contrasting a simple rule by CAL. However, while descriptively it seems like rule language did indeed facilitate XOR extrapolation as predicted by Hypothesis 1.2 - with 7.5% more extrapolated stimuli on average and 10% more persons classified as having learned a full XOR solution (see table \@ref(tab:h1-tbl-summary)) - such an effect can not be confirmed by the conducted analyses. Furthermore, an effect of rule instructions on the learning of (XOR) rules predicted by hypothesis 2.1 can also not be confirmed. A simple explanation might be that XOR-extrapolating novel stimuli might just be a matter of random chance. This is most unlikely, though, since that would not only contradict the findings of @CK17 but also those of @Kurtz2013 and to some extent even the original findings of @Shepard1961 as well as the replication done by @Nosofsky1994, all of whom did a whole array of experiments with occasionally rather large sample sizes and consistent results. This also runs counter to the findings of this study itself, which also shows very distinct and decisive response patterns between proximators and extrapolators, not only in categorical decisions but also in probability judgements (see figures \@ref(fig:h1-extrapolations) and \@ref(fig:xplore-prob) respectively). Another explanation might lie in CAL's similarity/contrast parameter $\gamma$: Simulations of the models performance on the same task with different $\gamma$ values show that stimulus extrapolation happens only in a rather narrow range of -1.5 to -2.5 [@Schlegelmilch2021]. Beyond that range and most noteworthy with values even smaller than -2.5, interpolations are more likely to occur. This means, smaller values translate to stronger contrasting of categories (stimuli get extrapolated) but only to a certain threshold, where contrasting becomes too strong, so to speak, such that an extrapolated stimulus on the lower-right corner of the stimulus grid does not get generalized to the adjacent stimuli. Put otherwise: contextual modulation does not get activated (see @Schlegelmilch2021, p. 48). From that point of view and with regard to the experimental manipulation this can mean several things. For one, the rule instructions might simply not have enough of an impact to trigger contextual modulation. For the other though, it might be the exact opposite and paricipants didn't pay attention to any conextual clues, memorizing stimuli in order to avoid making mistakes and then resorting to classifying by similarity in the transfer phase.

Another issue in the presented results is the astonishingly low number of xor-extrapolators over the whole dataset. Only 24 of 192 people (12.5\%) extrapolated to a full XOR solution, in comparison to the previous results by @CK17, who reported 9 of 30 (30\%) and 14 of 31 (45\%). One reason may be attributed, at least in part, to a methodical flaw in the execution of the experiment that was discovered only after data collection regarding the randomization of stimulus presentation: the unencountered stimuli during training were always the same for all participants across all conditions (always "big and black"). Are more methodically sound and proper operationalisation is to randomly assign one of the adjacent quadrants as critical targets or even rotate through the whole set (think: keep the structure as in fig, \@ref(fig:ch2-stimuli) (B) but rotate the set as depicted in (A)). It remains unclear whether such a procedure (or the lack thereof) had any ifluence on the results, but it's not unreasonable to assume that some bias had been introduced to the participants behaviour. Follow-up studies using a similar setup are advised to implement such a randomization. 


**Conclusion**

This thesis was able to confirm one of the major assumptions of the recent Catgeory Abstraction Learning framework: Establishing a simple rule and then adding contextual clues leads to contrasting that rule. Despite overwhelming evidence for the influence of rule instuctions in the literature, this study was not able to find any. However, the mechanism responsible for that might be the same that is able to produce such an effect and future research on the topic is advised to look into that.




<!-- For this experiment, participants were asked: "Wie hoch schätzt du die Wahrscheinichkeit ein, dass das gezeigte Beispiel entweder zu Nobz oder zu Grot gehört?" (_"How high do you estimate the probability of the shown example belonging to either Nobz or Grot?"_). Note the difference between "how high do you estimate" and the more neutral "what do you estimate": although this is common german phrasing, it might nonetheless introduce some unconscious response bias. Additionally, people have a very intuitive, but ultimately somewhat distorted (or even simply wrong) understanding of probability (literally everything Gigerenzer did since 1990 or so). For further investigations into the matther it is therefore advised to rephrase the question to: "How certain are you of the given example to belong to either Nobz or Grot?"^[which is indeed closer to what people actually have in mind when people are prompted to deal with probability] Thereby avoiding any potential bias and different perceptions/understandings of the concept of probability. -->

<!-- sth sth attention & german grammar: zum einen sind **Nobz** und **Grot** bei der ersten nennung bold ("rule(s)" nie) und zum anderen lautet der satz am ende: "Gib dein Bestes, um die Regeln für Kategorien Nobz und Grot zu meistern!" - soll heißen, im deutschen steht das Verb am ende und das nomen am anfang, auf englisch andersherum, was wiederum die aufmerksamkeit und/oder emphasis auf "Regeln" oder "meistern" lenken/legen könnte (siehe: "Do your best to master the rules for categories N & G.") -->
